{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Exercise with Spark _Olusegun AJOSE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kt4dQPvEVEHm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zBGR8Bv8PMtp"
      },
      "outputs": [],
      "source": [
        "# innstall java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "4EikmM9nPPdZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz"
      ],
      "metadata": {
        "id": "5ApuuTo9RAl4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set your spark folder to your system path environment. \n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\""
      ],
      "metadata": {
        "id": "wL7FKWh-RAjg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install findspark using pip\n",
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "lJIwWZ6wRAgs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qOiagWo9RAeL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark"
      ],
      "metadata": {
        "id": "A_GvJiBMPPbC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "findspark.init()\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "id": "hkiWD9sxPPYw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName('SMSSpamCollection').getOrCreate()"
      ],
      "metadata": {
        "id": "HOQHyHV-PPWo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.options(inferSchema='True',delimiter='\\t').csv(\"SMSSpamCollection.csv\")\n",
        "# df = spark.read.options(inferSchema='True', ).csv(\"spam.csv\")\n",
        "\n",
        "## Rename the columns\n",
        "df = df.withColumnRenamed(\"_c0\", \"class\").withColumnRenamed(\"_c1\", \"text\")\n",
        "# df = df.withColumnRenamed(\"_v1\", \"class\").withColumnRenamed(\"_v2\", \"text\")\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJ51pSpGz1f1",
        "outputId": "40f7683c-8801-4e72-9fae-4415bef749e4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- class: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oDZ9eUmB2WMH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a new length feature (new column w/ the length of the text)"
      ],
      "metadata": {
        "id": "1uwg52j74AF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "df = df.withColumn(\"textLength\", F.length(\"text\"))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDcWlmPK1F2U",
        "outputId": "7318b708-a4b9-41ce-91f8-3f72ea19d75c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+----------+\n",
            "|class|                text|textLength|\n",
            "+-----+--------------------+----------+\n",
            "|  ham|Go until jurong p...|       111|\n",
            "|  ham|Ok lar... Joking ...|        29|\n",
            "| spam|Free entry in 2 a...|       155|\n",
            "|  ham|U dun say so earl...|        49|\n",
            "|  ham|Nah I don't think...|        61|\n",
            "| spam|FreeMsg Hey there...|       147|\n",
            "|  ham|Even my brother i...|        77|\n",
            "|  ham|As per your reque...|       160|\n",
            "| spam|WINNER!! As a val...|       157|\n",
            "| spam|Had your mobile 1...|       154|\n",
            "|  ham|I'm gonna be home...|       109|\n",
            "| spam|SIX chances to wi...|       136|\n",
            "| spam|URGENT! You have ...|       155|\n",
            "|  ham|I've been searchi...|       196|\n",
            "|  ham|I HAVE A DATE ON ...|        35|\n",
            "| spam|XXXMobileMovieClu...|       149|\n",
            "|  ham|Oh k...i'm watchi...|        26|\n",
            "|  ham|Eh u remember how...|        81|\n",
            "|  ham|Fine if thats th...|        56|\n",
            "| spam|England v Macedon...|       155|\n",
            "+-----+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What do you notice ?"
      ],
      "metadata": {
        "id": "S4CzYynV3-0B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Visual inspection shows that the average length of the spam messages is longer."
      ],
      "metadata": {
        "id": "_Bxe_jf66n5B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## We can confirm this below\n",
        "df.groupby('class').mean().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVLpTb2m1Fy6",
        "outputId": "f05fe927-31ab-49b5-d3d3-b1470f1c5168"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------------+\n",
            "|class|  avg(textLength)|\n",
            "+-----+-----------------+\n",
            "|  ham|71.45431945307645|\n",
            "| spam|138.6706827309237|\n",
            "+-----+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create feature transformers"
      ],
      "metadata": {
        "id": "SlKd5AzH4Q9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer, NGram\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n"
      ],
      "metadata": {
        "id": "79OE9LdS1FnF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use Pipeline to create a data pre-processing pipeline as follows "
      ],
      "metadata": {
        "id": "Ar5HB_F39P7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Use VectorAssembler to create an assembler of tf_idf feature with length \n",
        "## (column should be called ‘features’)\n",
        "\n",
        "## Change to True to use NGRAM\n",
        "use_ngram = False\n",
        "\n",
        "# Indexing class column to a numeric label (output column should be called ‘label’)\n",
        "data_to_num = StringIndexer(inputCol='class', outputCol='label')\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\n",
        "\n",
        "# # • Ngrams (try with and without ngrams ; n=2)\n",
        "if use_ngram:\n",
        "  ngram = NGram(n=2, inputCol = \"token_text\", outputCol=\"ngrams\")\n",
        "  # • Count vertorization\n",
        "  count_vec = CountVectorizer(inputCol='ngrams', outputCol='count_vec_stop')\n",
        "\n",
        "  # • Stop words removal\n",
        "  # stop_remove = StopWordsRemover(inputCol='ngrams', outputCol='stop_tokens')\n",
        "else:\n",
        "  # • Stop words removal\n",
        "  stop_remove = StopWordsRemover(inputCol='token_text', outputCol='stop_tokens')\n",
        "  # • Count vertorization\n",
        "  count_vec = CountVectorizer(inputCol='stop_tokens', outputCol='count_vec_stop')\n",
        "\n",
        "\n",
        "# • IDF\n",
        "idf = IDF(inputCol=\"count_vec_stop\", outputCol=\"tf_idf\")\n",
        "\n",
        "# • Vector assembling\n",
        "vec_assembler = VectorAssembler(inputCols=['tf_idf', 'textLength'], outputCol='features')"
      ],
      "metadata": {
        "id": "7hka73y81FjV"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LL9QNUMm1FLF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transform the data DataFrame through the pipeline (last column should be called ‘features’)"
      ],
      "metadata": {
        "id": "Kru5OfBT90Ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "spam_pipe = Pipeline(stages=[data_to_num, tokenizer, stop_remove, count_vec, idf, vec_assembler])\n",
        "spam_cleaner = spam_pipe.fit(df)\n",
        "spam_data_clean = spam_cleaner.transform(df)"
      ],
      "metadata": {
        "id": "cxI7_ADy1E-2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zzXCGdyy1E7f"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import NaiveBayse model from pyspark.ml.classification"
      ],
      "metadata": {
        "id": "wfwq4GM8AF19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "nb = NaiveBayes(featuresCol='features', labelCol='label')"
      ],
      "metadata": {
        "id": "wdeN6KRX1E3-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a training DataFrame by selection the “label” and “features” column"
      ],
      "metadata": {
        "id": "NF_CyIbs1E1b"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spam_data_clean = spam_data_clean.select(['label','features'])\n",
        "spam_data_clean.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOlgwCUy1EwM",
        "outputId": "846fe088-ee09-4133-9a6c-0840f5159e9b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(13424,[7,11,31,6...|\n",
            "|  0.0|(13424,[0,24,297,...|\n",
            "|  1.0|(13424,[2,13,19,3...|\n",
            "|  0.0|(13424,[0,70,80,1...|\n",
            "|  0.0|(13424,[36,134,31...|\n",
            "|  1.0|(13424,[10,60,139...|\n",
            "|  0.0|(13424,[10,53,103...|\n",
            "|  0.0|(13424,[125,184,4...|\n",
            "|  1.0|(13424,[1,47,118,...|\n",
            "|  1.0|(13424,[0,1,13,27...|\n",
            "|  0.0|(13424,[18,43,120...|\n",
            "|  1.0|(13424,[8,17,37,8...|\n",
            "|  1.0|(13424,[13,30,47,...|\n",
            "|  0.0|(13424,[39,96,217...|\n",
            "|  0.0|(13424,[552,1697,...|\n",
            "|  1.0|(13424,[30,109,11...|\n",
            "|  0.0|(13424,[82,214,47...|\n",
            "|  0.0|(13424,[0,2,49,13...|\n",
            "|  0.0|(13424,[0,74,105,...|\n",
            "|  1.0|(13424,[4,30,33,5...|\n",
            "+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Use random split to split the training data\n",
        "(train_data, test_data) = spam_data_clean.randomSplit([0.8, 0.2])\n",
        "\n",
        "## Train your model (fit method)\n",
        "predictor = nb.fit(train_data)\n"
      ],
      "metadata": {
        "id": "J1IKg5nGA2mB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Apply your model to test data\n",
        "test_prediction = predictor.transform(test_data)\n",
        "test_prediction.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_XlgKKnA2jg",
        "outputId": "7683631a-9dae-4ee4-df59-5faf19cec11e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+--------------------+--------------------+----------+\n",
            "|label|            features|       rawPrediction|         probability|prediction|\n",
            "+-----+--------------------+--------------------+--------------------+----------+\n",
            "|  0.0|(13424,[0,1,2,7,8...|[-797.10741000107...|[1.0,2.3171984914...|       0.0|\n",
            "|  0.0|(13424,[0,1,4,50,...|[-830.27854819841...|[1.0,1.0080865887...|       0.0|\n",
            "|  0.0|(13424,[0,1,7,8,1...|[-884.32728572833...|[1.0,4.1959570815...|       0.0|\n",
            "|  0.0|(13424,[0,1,14,31...|[-216.63456340940...|[1.0,7.1110318817...|       0.0|\n",
            "|  0.0|(13424,[0,1,14,78...|[-687.99565244781...|[1.0,9.5171867055...|       0.0|\n",
            "|  0.0|(13424,[0,1,15,20...|[-670.92985171347...|[1.0,7.7938013580...|       0.0|\n",
            "|  0.0|(13424,[0,1,21,27...|[-751.11958677349...|[1.0,1.6237466631...|       0.0|\n",
            "|  0.0|(13424,[0,1,27,88...|[-1532.9239280363...|[0.34398256936550...|       1.0|\n",
            "|  0.0|(13424,[0,1,498,5...|[-320.05306471727...|[0.99999999999835...|       0.0|\n",
            "|  0.0|(13424,[0,1,3657,...|[-128.80309706194...|[0.99996515137051...|       0.0|\n",
            "|  0.0|(13424,[0,2,3,6,9...|[-3319.3748932881...|[1.0,3.0319927379...|       0.0|\n",
            "|  0.0|(13424,[0,2,4,5,1...|[-2503.5222801728...|[1.0,6.4108310113...|       0.0|\n",
            "|  0.0|(13424,[0,2,7,8,1...|[-746.51835193052...|[1.0,1.2709666936...|       0.0|\n",
            "|  0.0|(13424,[0,2,7,8,3...|[-1150.4219325614...|[1.0,1.3289800946...|       0.0|\n",
            "|  0.0|(13424,[0,2,7,11,...|[-855.31102735165...|[1.0,5.7377185843...|       0.0|\n",
            "|  0.0|(13424,[0,2,7,43,...|[-592.83973122181...|[1.0,2.2521430719...|       0.0|\n",
            "|  0.0|(13424,[0,2,7,60,...|[-1357.7819229082...|[1.0,1.7202659368...|       0.0|\n",
            "|  0.0|(13424,[0,2,8,12,...|[-1344.2411096695...|[1.0,1.1025837413...|       0.0|\n",
            "|  0.0|(13424,[0,2,8,28,...|[-1314.6268106997...|[1.0,1.1440132037...|       0.0|\n",
            "|  0.0|(13424,[0,2,10,13...|[-4854.0629355799...|           [1.0,0.0]|       0.0|\n",
            "+-----+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate accuracy using MulticlassClassificationEvaluator from pyspark.ml.evaluation"
      ],
      "metadata": {
        "id": "3GecoJ27CJn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "mcc_eval_acc = MulticlassClassificationEvaluator()\n",
        "model_acc = mcc_eval_acc.evaluate(test_prediction)\n",
        "\n",
        "print(\"Model Accuracy: {:.2f}\".format(model_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgbblN8CA2en",
        "outputId": "6a036c2b-8b3c-45f3-dd4c-287e508b3734"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "p1d6PX4LA2bq"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results\n",
        "#### Model Accuracy was 92% without NGrams\n",
        "#### Model Accuracy was 62% with NGrams\n"
      ],
      "metadata": {
        "id": "NTUg44OOFq_o"
      }
    }
  ]
}